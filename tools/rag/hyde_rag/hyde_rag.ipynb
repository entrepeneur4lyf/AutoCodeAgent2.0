{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Hypothetical Document Embedding (HyDE) - RAG Retrieval\n",
       "\n",
       "This notebook presents a modern approach to document retrieval by expanding short queries into full-length, detailed texts. The method uses the Hypothetical Document Embedding technique to create a better match between the query representation and document embeddings."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## System Introduction\n",
       "\n",
       "In this system, a language model transforms a userâ€™s query into an extensive, hypothetical document. This document then serves as an enriched query to search through a vector space, improving the alignment with the stored documents."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Underlying Concept\n",
       "\n",
       "Standard retrieval methods often fail when dealing with the disparity between concise queries and detailed documents. By generating a comprehensive hypothetical document, this method fills the gap and leads to more accurate and context-aware search results. This approach is particularly beneficial for multifaceted or complex queries."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## System Components\n",
       "\n",
       "1. **Document Ingestion and Chunking:** Leverage the llama_index RAG approach to ingest and group PDF documents or accept inputs directly from chat windows, splitting content into manageable segments.\n",
       "2. **Vector Database Management:** Use the llama_index implementation to manage the vector database, enabling efficient indexing and similarity search.\n",
       "3. **Hypothetical Text Generation:** A language model (like GPT-4o) generates an extensive hypothetical document from the query.\n",
       "4. **Customized Retrieval Module:** A dedicated module manages the generation of hypothetical texts and retrieves documents based on vector similarity using the llama_index system."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## How It Works\n",
       "\n",
       "### Text Preprocessing and Vector Indexing\n",
       "\n",
       "- Use llama_index to ingest and chunk the documents. Read all the documentation in the llama_index folder.\n",
       "\n",
       "### Creating the Hypothetical Text\n",
       "\n",
       "- Use a language model to create an extensive document that responds to the query.\n",
       "- Employ a prompt template to enforce detailed content with a fixed character length matching the chunks.\n",
       "\n",
       "### Retrieval Mechanism\n",
       "\n",
       "1. Generate a detailed text from the input query using the language model.\n",
       "2. Use the generated text as the search key within the vector index.\n",
       "3. Retrieve the documents that are most similar to this hypothetical text."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Diagram\n",
        "\n",
        "![HyDE RAG Workflow](./hyde_rag_workflow.png)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## System Advantages\n",
       "\n",
       "- **Enhanced Query Representation:** Transforming short queries into detailed texts improves match quality.\n",
       "- **Customizable Parameters:** Adjust chunk sizes, overlaps, and the number of returned documents based on your needs.\n",
       "- **Integration with Advanced Models:** Utilizes powerful language models like GPT-4 along with robust vector representations."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Practical Benefits\n",
       "\n",
       "- **Better Result Relevance:** Detailed queries capture the nuances of the search intent, leading to more precise document matches.\n",
       "- **Complex Query Resolution:** Effective for queries with multiple aspects or in-depth information requirements.\n",
       "- **Versatility Across Domains:** Suitable for specialized fields such as legal research, academic studies, and other advanced retrieval systems."
      ]
     },
     {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
         "## Implementation Insights\n",
         "\n",
         "- **retrieve.py** file has function that retrieves the documents from the vector database.\n",
         "- **prompts.py** file contains the prompt for the language model to generate the hypothetical document.\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Conclusion\n",
          "\n",
          "The Hypothetical Document Embedding (HyDE) - RAG Retrieval system offers a transformative approach to document search by expanding concise queries into detailed, enriched texts. This method bridges the gap between short user queries and comprehensive document representations, ensuring improved alignment during the retrieval process. By leveraging advanced text preprocessing, robust vector indexing, and a sophisticated retrieval mechanism, the system enhances relevance and accuracy in search results. Ultimately, HyDE - RAG Retrieval stands out as a versatile and effective solution for complex queries across various domains."
        ]
      }
    ],
    "metadata": {
        "kernelspec": {
          "display_name": "Python 3",
          "language": "python",
          "name": "python3"
        },
        "language_info": {
          "name": "python"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 2
   }
   