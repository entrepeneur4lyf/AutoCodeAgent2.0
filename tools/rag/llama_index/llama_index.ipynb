{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index RAG Retrieval and Ingestion\n",
    "\n",
    "This notebook presents a straightforward implementation of a Retrieval-Augmented Generation (RAG) system using the llama_index library. The system ingests documents, creates a vector index, and retrieves the most relevant content based on a query. In addition to the techniques above, the agent now integrates the Llama Index for even more advanced data retrieval and ingestion, enhancing its ability to work with complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Introduction\n",
    "\n",
    "In this system, documents are ingested from either text strings or a directory, indexed using llama_index, and later retrieved using a query. A language model is used in other parts of the overall system (not shown here) to generate queries or hypothetical documents. With Llama Index now added as a default tool, it is possible to customize the execution of ingestion and retrieval code by adding additional parameters provided by the Llama Index documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underlying Concept\n",
    "\n",
    "Traditional retrieval methods may require heavy customization when working with large document corpora. By leveraging llama_index, the system automatically handles document ingestion, vector indexing, and similarity search. This creates a robust, scalable retrieval solution that easily integrates with modern language models. Furthermore, integrating the Llama Index allows for advanced customization and improved handling of complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Components\n",
    "\n",
    "1. **Document Retrieval Module:** Loads a persisted llama_index vector store and retrieves documents relevant to a query. \n",
    "   - *Example Prompt for Retrieval:* \"Find the latest market trends using the Llama Index.\"\n",
    "\n",
    "2. **Text Ingestion Module:** Ingests a list of raw text strings into the vector store. It creates or appends to an existing index and persists the update.\n",
    "   - *Example Prompt for Ingestion:* \"Find the latest market trends from the web and save it in the database using the Llama Index.\"\n",
    "\n",
    "3. **Corpus Ingestion Module:** Reads documents from a specified directory using the SimpleDirectoryReader, then creates or updates the llama_index vector store.\n",
    "   - **Bulk Ingestion:** You can ingest a corpus by directly uploading files to the `/tools/rag/llama_index/corpus` folder. To start the batch processing, simply make an API call:\n",
    "     ```bash\n",
    "     curl -X POST http://localhost:5000/llama-index-ingest-corpus\n",
    "     ```\n",
    "     The script processes the information in the files, transferring it to the vector and graph database. By default, the SimpleDirectoryReader attempts to read any files it encounters (treating them as text), and explicitly supports file types such as:\n",
    "     - .csv (Comma-Separated Values)\n",
    "     - .docx (Microsoft Word)\n",
    "     - .epub (EPUB eBook format)\n",
    "     - .hwp (Hangul Word Processor)\n",
    "     - .ipynb (Jupyter Notebook)\n",
    "     - .jpeg, .jpg (JPEG image)\n",
    "     - .mbox (MBOX email archive)\n",
    "     - .md (Markdown)\n",
    "     - .mp3, .mp4 (Audio and video)\n",
    "     - .pdf (Portable Document Format)\n",
    "     - .png (Portable Network Graphics)\n",
    "     - .ppt, .pptm, .pptx (Microsoft PowerPoint)\n",
    "     \n",
    "     For more details, refer to the [SimpleDirectoryReader Documentation](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### 1. Text Preprocessing and Vector Indexing\n",
    "\n",
    "- The retrieval module loads the persisted index from disk using a specified directory.\n",
    "- Ingestion modules process raw text or directory files and update the vector store accordingly.\n",
    "- Llama Index parameters can be customized to suit specific retrieval or ingestion needs, providing flexibility for various data types.\n",
    "\n",
    "### 2. Retrieval Mechanism\n",
    "\n",
    "- A query is submitted to the retrieval module, which loads the index and searches for the most similar nodes/documents.\n",
    "- The retrieved nodes are then returned as a list of text strings.\n",
    "- *Example Prompt:* \"Find the latest market trends using the Llama Index.\"\n",
    "\n",
    "### 3. Corpus Ingestion\n",
    "\n",
    "- The corpus ingestion module reads all documents from a directory, ingests them into the index, and persists the updated index.\n",
    "- For bulk ingestion, files placed in the designated corpus folder are processed via an API call, allowing the system to update the vector and graph databases automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Advantages\n",
    "\n",
    "- **Automated Vector Management:** llama_index simplifies document ingestion and indexing.\n",
    "- **Scalability:** Easily ingest and index new documents from text or entire corpora without significant re-engineering.\n",
    "- **Seamless Integration:** Works alongside modern language models for end-to-end RAG pipelines.\n",
    "- **Advanced Customization:** With Llama Index, users can adjust parameters for retrieval and ingestion to meet specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Benefits\n",
    "\n",
    "- **Improved Retrieval Accuracy:** Efficient similarity search based on vector representations.\n",
    "- **Flexibility:** Supports ingestion from both raw text and document directories, including bulk ingestion via API calls.\n",
    "- **Easy Deployment:** By persisting the index to disk, the system can be restarted or updated incrementally.\n",
    "\n",
    "With these RAG techniques, AutoCodeAgent 2.0 transforms the way you interact with data, making it easier than ever to store, retrieve, and analyze information. Whether you're working on simple tasks or tackling complex data challenges, these tools empower your workflow and unlock new possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Insights\n",
    "\n",
    "- The **retrieve.py** module leverages llama_index's `StorageContext` and `VectorIndexRetriever` to load and search the index.\n",
    "- The **ingest.py** module creates or updates the index using a list of text strings and persists changes using the storage context.\n",
    "- The **ingest_corpus.py** module uses the `SimpleDirectoryReader` to load files from a directory, then either creates a new index or appends to an existing one before persisting it.\n",
    "- Bulk ingestion is supported via an API endpoint, enabling the processing of various file types as detailed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "**LLAMA_INDEX_TOP_K_RAG_RETRIEVE:**\n",
    "This environment variable controls the number of top documents retrieved during a query execution in the Llama Index RAG system. In the `retrieve_documents` function, it sets the `similarity_top_k` parameter for the `VectorIndexRetriever`. Adjusting this parameter allows you to balance between retrieving more documents for higher recall and fewer, more relevant documents for higher precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Llama Index RAG Retrieval and Ingestion system presents a comprehensive solution for managing and querying large document corpora. By automatically ingesting documents, creating robust vector indices, and enabling advanced similarity search, this system simplifies complex data retrieval tasks. With modular components for document ingestion, customized retrieval, and bulk processing via API endpoints, it offers both scalability and flexibility. Overall, this approach enhances data accessibility and streamlines the integration of modern language models into real-world applications, paving the way for more efficient and accurate information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
