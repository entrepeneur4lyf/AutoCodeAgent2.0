{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index RAG Retrieval and Ingestion\n",
    "\n",
    "This notebook presents a straightforward implementation of a Retrieval-Augmented Generation (RAG) system using the llama_index library. The system ingests documents, creates a vector index, and retrieves the most relevant content based on a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Introduction\n",
    "\n",
    "In this system, documents are ingested from either text strings or a directory, indexed using llama_index, and later retrieved using a query. A language model is used in other parts of the overall system (not shown here) to generate queries or hypothetical documents, but this implementation focuses on the vector database management with llama_index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underlying Concept\n",
    "\n",
    "Traditional retrieval methods may require heavy customization when working with large document corpora. By leveraging llama_index, the system automatically handles document ingestion, vector indexing, and similarity search. This creates a robust, scalable retrieval solution that easily integrates with modern language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Components\n",
    "\n",
    "1. **Document Retrieval Module:** Loads a persisted llama_index vector store and retrieves documents relevant to a query.\n",
    "2. **Text Ingestion Module:** Ingests a list of raw text strings into the vector store. It creates or appends to an existing index and persists the update.\n",
    "3. **Corpus Ingestion Module:** Reads documents from a specified directory using the SimpleDirectoryReader, then creates or updates the llama_index vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### 1. Text Preprocessing and Vector Indexing\n",
    "\n",
    "- The retrieval module loads the persisted index from disk using a specified directory.\n",
    "- Ingestion modules process raw text or directory files and update the vector store accordingly.\n",
    "\n",
    "### 2. Retrieval Mechanism\n",
    "\n",
    "- A query is submitted to the retrieval module, which loads the index and searches for the most similar nodes/documents.\n",
    "- The retrieved nodes are then returned as a list of text strings.\n",
    "\n",
    "### 3. Corpus Ingestion\n",
    "\n",
    "- The corpus ingestion module reads all documents from a directory, ingests them into the index, and persists the updated index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Advantages\n",
    "\n",
    "- **Automated Vector Management:** llama_index simplifies document ingestion and indexing.\n",
    "- **Scalability:** Easily ingest and index new documents from text or entire corpora without significant re-engineering.\n",
    "- **Seamless Integration:** Works alongside modern language models for end-to-end RAG pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Benefits\n",
    "\n",
    "- **Improved Retrieval Accuracy:** Efficient similarity search based on vector representations.\n",
    "- **Flexibility:** Supports ingestion from both raw text and document directories.\n",
    "- **Easy Deployment:** By persisting the index to disk, the system can be restarted or updated incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Insights\n",
    "\n",
    "- The **retrieve** module leverages llama_index's `StorageContext` and `VectorIndexRetriever` to load and search the index.\n",
    "- The **ingest_texts** module creates or updates the index using a list of text strings and persists changes using the storage context.\n",
    "- The **ingest_corpus** module uses the `SimpleDirectoryReader` to load files from a directory, then either creates a new index or appends to an existing one before persisting it.\n",
    "\n",
    "Each component is designed for clarity and robustness, ensuring that the vector database is always up-to-date with minimal manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This simple llama_index RAG system rethinks document retrieval by automating ingestion and indexing of documents. By utilizing the llama_index library, the system efficiently manages vector storage, making it well-suited for integration with language models to achieve enhanced retrieval relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "\n",
    "Below, the code for each external module is automatically displayed using `%pycat`. This allows you to review the code without running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pycat retrieve.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat ingest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat ingest_corpus.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
